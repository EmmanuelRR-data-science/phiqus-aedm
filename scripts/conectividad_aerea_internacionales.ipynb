{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtB0Rr4ZgZJW"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ======================================================\n",
    "# Datatur YTD -> Pie Plotly con estilo corporativo\n",
    "# ADAPTADO A PLOTLY/STREAMLIT con estilos dinámicos\n",
    "# ======================================================\n",
    "\n",
    "import io, re, zipfile, unicodedata\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import plotly.graph_objects as go\n",
    "import streamlit as st \n",
    "\n",
    "# ----------------- Constantes -----------------\n",
    "DATATUR_ZIP_URL = \"https://datatur.sectur.gob.mx/Documentos%20compartidos/CUADRO_DGAC.zip\"\n",
    "UA = {\"User-Agent\":\"Mozilla/5.0 (compatible; DataPipeline/1.4; +https://github.com)\"}\n",
    "\n",
    "# Tipografía preferida\n",
    "FONT_STACK = \"Aptos Light, Aptos, Arial, sans-serif\"\n",
    "\n",
    "# Tamaño y márgenes para exportación\n",
    "FIG_WIDTH = 800\n",
    "FIG_HEIGHT = 500\n",
    "LEFT_MARGIN = 220\n",
    "\n",
    "# Expresiones p/ detectar cuadro y columnas\n",
    "TITLE_RE = re.compile(\n",
    "    r\"pasajeros\\s+transportados\\s+en\\s+vuelos\\s+internacionales\\s+por\\s+principales\\s+aerolineas\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "AEROL_RE = re.compile(r\"aerol[ií]neas?\", re.IGNORECASE)\n",
    "\n",
    "YTD_TAG_RE = re.compile(\n",
    "    r\"(ene(?:ro)?|feb|mar|abr|may|jun|jul|ago|sept?|oct|nov|dic)\"\n",
    "    r\".{0,6}\"\n",
    "    r\"(?:a|–|-|al)?\"\n",
    "    r\".{0,6}\"\n",
    "    r\"(ene(?:ro)?|feb|mar|abr|may|jun|jul|ago|sept?|oct|nov|dic)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "YEAR_RE = re.compile(r\"\\(?\\b(20\\d{2})\\b\\)?\")\n",
    "PART_RE = re.compile(r\"participaci[oó]n?\\s*%?\", re.IGNORECASE)\n",
    "\n",
    "REGION_KEYS = {\n",
    "    \"total mexicanas\": \"Mexicanas\",\n",
    "    \"total estadounidenses\": \"Estadounidenses\",\n",
    "    \"total canadienses\": \"Canadienses\",\n",
    "    \"total europeas\": \"Europeas\",\n",
    "    \"total centro y sudamericanas\": \"Centro/Sudamericanas\",\n",
    "    \"total asiaticas\": \"Asiáticas\",\n",
    "    \"total\": \"TOTAL\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Adaptación de Estilo para Streamlit\n",
    "# ---------------------------------------------\n",
    "DEFAULT_COLOR_TOPS = [\"#889064\", \"#0e1c2c\", \"#ff9f18\"]\n",
    "DEFAULT_REST_PALETTE = [\"#b5b9ad\", \"#2a3b55\", \"#ffb955\", \"#d0d4c8\", \"#3d4f6b\", \"#ffd08a\", \"#7a8467\", \"#1a2a40\"]\n",
    "\n",
    "PALETTE = globals().get('active_palette', DEFAULT_COLOR_TOPS + DEFAULT_REST_PALETTE)\n",
    "FONT = globals().get('active_font', FONT_STACK)\n",
    "\n",
    "FONT_FAMILY_PLOTLY = FONT\n",
    "COLOR_TOPS = PALETTE[:3]\n",
    "REST_PALETTE = PALETTE[3:]\n",
    "\n",
    "\n",
    "# ----------------- Utilerías -----------------\n",
    "def zstrip(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFD\", str(s))\n",
    "    s = \"\".join(c for c in s if unicodedata.category(c) != \"Mn\")\n",
    "    s = re.sub(r\"\\s+\", \" \", s.strip().lower())\n",
    "    s = s.replace(\"t o t a l\", \"total\")\n",
    "    return s\n",
    "\n",
    "def download_zip(url: str) -> bytes:\n",
    "    r = requests.get(url, headers=UA, timeout=90)\n",
    "    r.raise_for_status()\n",
    "    return r.content\n",
    "\n",
    "def ffill_list(lst: List[str]) -> List[str]:\n",
    "    out, cur = [], \"\"\n",
    "    for x in lst:\n",
    "        s = (str(x) if x is not None else \"\").strip()\n",
    "        if s.lower() != \"nan\" and s != \"\":\n",
    "            cur = s\n",
    "        out.append(cur)\n",
    "    return out\n",
    "\n",
    "def find_title_blocks(df: pd.DataFrame) -> List[Tuple[int,int]]:\n",
    "    blocks = []\n",
    "    for i in range(df.shape[0]):\n",
    "        for j in range(df.shape[1]):\n",
    "            val = df.iat[i,j]\n",
    "            if pd.isna(val): continue\n",
    "            if TITLE_RE.search(zstrip(val)):\n",
    "                blocks.append((i,j))\n",
    "    return blocks\n",
    "\n",
    "def reconstruct_with_two_header_rows(raw: pd.DataFrame, hdr_row: int, sec_row: int) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    max_cols = raw.shape[1]\n",
    "    top = [raw.iat[hdr_row, j] if j < raw.shape[1] else \"\" for j in range(max_cols)]\n",
    "    bot = [raw.iat[sec_row, j] if j < raw.shape[1] else \"\" for j in range(max_cols)]\n",
    "    top = [re.sub(r\"\\s+\", \" \", str(x).strip()) for x in top]\n",
    "    bot = [re.sub(r\"\\s+\", \" \", str(x).strip()) for x in bot]\n",
    "    top_ff = ffill_list(top)\n",
    "    cols = []\n",
    "    for j, (t, b) in enumerate(zip(top_ff, bot)):\n",
    "        if b and (YEAR_RE.search(b) or PART_RE.search(b) or \"Variaci\" in b):\n",
    "            cols.append((f\"{t} {b}\").strip() if t else b)\n",
    "        else:\n",
    "            cols.append(t if t else (b if b else f\"col_{j}\"))\n",
    "    start = sec_row + 1\n",
    "    end = start\n",
    "    while end < raw.shape[0]:\n",
    "        c0 = zstrip(raw.iat[end,0]) if end < raw.shape[0] else \"\"\n",
    "        if \"fuente:\" in c0 or c0.startswith(\"notas:\"):\n",
    "            break\n",
    "        end += 1\n",
    "    body = raw.iloc[start:end, :len(cols)].copy()\n",
    "    body.columns = cols[:body.shape[1]]\n",
    "    return body, cols\n",
    "\n",
    "def read_sheet_candidate(xls: pd.ExcelFile, sheet: str) -> Optional[pd.DataFrame]:\n",
    "    raw = pd.read_excel(xls, sheet_name=sheet, header=None, engine=\"openpyxl\")\n",
    "    blocks = find_title_blocks(raw)\n",
    "    if blocks:\n",
    "        for (r, _c) in blocks:\n",
    "            for offset in range(1, 6):\n",
    "                hdr_row = r + offset\n",
    "                if hdr_row >= raw.shape[0]: continue\n",
    "                row_vals = raw.iloc[hdr_row,:].astype(str).tolist()\n",
    "                if any(AEROL_RE.search(v) for v in row_vals):\n",
    "                    sec_row = hdr_row + 1 if hdr_row + 1 < raw.shape[0] else hdr_row\n",
    "                    body, cols = reconstruct_with_two_header_rows(raw, hdr_row, sec_row)\n",
    "                    aerol_cols = [k for k in body.columns if AEROL_RE.search(k)]\n",
    "                    if not aerol_cols:\n",
    "                        best_col, best_hits = None, 0\n",
    "                        for k in body.columns:\n",
    "                            vals = body[k].astype(str).map(zstrip)\n",
    "                            hits = sum(v in REGION_KEYS for v in vals)\n",
    "                            if hits > best_hits:\n",
    "                                best_col, best_hits = k, hits\n",
    "                        if best_col and best_hits >= 5:\n",
    "                            body.rename(columns={best_col:\"Aerolíneas\"}, inplace=True)\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        body.rename(columns={aerol_cols[0]:\"Aerolíneas\"}, inplace=True)\n",
    "                    vals = body[\"Aerolíneas\"].astype(str).map(zstrip)\n",
    "                    body = body.loc[vals.isin(REGION_KEYS.keys())].copy()\n",
    "                    if not body.empty and body.shape[0] >= 6:\n",
    "                        return body\n",
    "    best_hits = 0; best = None\n",
    "    for j in range(raw.shape[1]):\n",
    "        vals = raw.iloc[:, j].astype(str).map(zstrip)\n",
    "        hits = vals.isin(REGION_KEYS.keys()).sum()\n",
    "        if hits > best_hits:\n",
    "            best_hits = hits; best = vals\n",
    "    if best_hits >= 5:\n",
    "        first_lab_row = int(best[best.isin(REGION_KEYS.keys())].index.min())\n",
    "        hdr_row = max(0, first_lab_row - 2)\n",
    "        sec_row = min(raw.shape[0]-1, hdr_row + 1)\n",
    "        body, cols = reconstruct_with_two_header_rows(raw, hdr_row, sec_row)\n",
    "        best_col, best_hits = None, 0\n",
    "        for k in body.columns:\n",
    "            vals = body[k].astype(str).map(zstrip)\n",
    "            hits = vals.isin(REGION_KEYS.keys()).sum()\n",
    "            if hits > best_hits:\n",
    "                best_col, best_hits = k, hits\n",
    "        if best_col and best_hits >= 5:\n",
    "            body.rename(columns={best_col:\"Aerolíneas\"}, inplace=True)\n",
    "            vals = body[\"Aerolíneas\"].astype(str).map(zstrip)\n",
    "            body = body.loc[vals.isin(REGION_KEYS.keys())].copy()\n",
    "            if not body.empty and body.shape[0] >= 6:\n",
    "                return body\n",
    "    return None\n",
    "\n",
    "def extract_year(text: str) -> Optional[int]:\n",
    "    if not isinstance(text, str): text = str(text)\n",
    "    m = YEAR_RE.search(text)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def pick_latest_ytd_columns(columns: List[str], *, verbose=False) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]:\n",
    "    cols = [str(c) for c in columns]\n",
    "    candidates = []\n",
    "    for c in cols:\n",
    "        if YTD_TAG_RE.search(c) and YEAR_RE.search(c):\n",
    "            yr = extract_year(c)\n",
    "            if yr: candidates.append((c, yr))\n",
    "    if candidates:\n",
    "        candidates.sort(key=lambda x: x[1])\n",
    "        latest_year = candidates[-1][1]\n",
    "        col_latest = next(c for c,y in candidates if y == latest_year)\n",
    "        prevs = [c for c,y in candidates if y < latest_year]\n",
    "        col_prev = prevs[-1] if prevs else None\n",
    "        col_part = None\n",
    "        for c in cols:\n",
    "            if str(latest_year) in c and PART_RE.search(c):\n",
    "                col_part = c; break\n",
    "        return col_prev, col_latest, col_part, latest_year\n",
    "\n",
    "    year_cols = []\n",
    "    for c in cols:\n",
    "        yr = extract_year(c)\n",
    "        if yr: year_cols.append((c, yr))\n",
    "    if year_cols:\n",
    "        year_cols.sort(key=lambda x: x[1])\n",
    "        latest_year = year_cols[-1][1]\n",
    "        col_latest = next(c for c,y in year_cols if y == latest_year)\n",
    "        prevs = [c for c,y in year_cols if y < latest_year]\n",
    "        col_prev = prevs[-1][0] if prevs else None\n",
    "        col_part = None\n",
    "        for c in cols:\n",
    "            if str(latest_year) in c and PART_RE.search(c):\n",
    "                col_part = c; break\n",
    "        return col_prev, col_latest, col_part, latest_year\n",
    "    return None, None, None, None\n",
    "\n",
    "def to_number(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.astype(str)\n",
    "        .str.replace(r\"[^\\d\\-,\\.]\", \"\", regex=True)\n",
    "        .str.replace(\".\", \"\", regex=False)\n",
    "        .str.replace(\",\", \"\", regex=False)\n",
    "        .replace({\"\": None, \"-\": None})\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "def get_series(df: pd.DataFrame, colname: str) -> pd.Series:\n",
    "    obj = df[colname]\n",
    "    if isinstance(obj, pd.DataFrame): return obj.iloc[:, -1]\n",
    "    return obj\n",
    "\n",
    "def _select_participation_series(df: pd.DataFrame, latest_year: int):\n",
    "    part_cols = [c for c in df.columns if PART_RE.search(str(c))]\n",
    "    if not part_cols: return None\n",
    "    year_cols = [c for c in part_cols if str(latest_year) in str(c)]\n",
    "    chosen = year_cols[-1] if year_cols else part_cols[-1]\n",
    "    ser = df[chosen]\n",
    "    if isinstance(ser, pd.DataFrame): ser = ser.iloc[:, -1]\n",
    "    return ser\n",
    "\n",
    "# ----------------- Pipeline principal -----------------\n",
    "@st.cache_data(ttl=3600)\n",
    "def run_datatur_analysis():\n",
    "    try:\n",
    "        content = download_zip(DATATUR_ZIP_URL)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fallo al descargar el ZIP de Datatur: {e}\")\n",
    "\n",
    "    zf = zipfile.ZipFile(io.BytesIO(content))\n",
    "    excel_files = [m for m in zf.infolist() if m.filename.lower().endswith((\".xlsx\", \".xls\"))]\n",
    "    if not excel_files:\n",
    "        raise RuntimeError(\"El ZIP no contiene archivos Excel.\")\n",
    "\n",
    "    parsed = None\n",
    "    for memb in excel_files:\n",
    "        try:\n",
    "            data = io.BytesIO(zf.read(memb))\n",
    "            xls = pd.ExcelFile(data, engine=\"openpyxl\")\n",
    "            for sh in xls.sheet_names:\n",
    "                body = read_sheet_candidate(xls, sh)\n",
    "                if body is not None:\n",
    "                    parsed = body; break\n",
    "            if parsed is not None: break\n",
    "        except Exception: pass\n",
    "\n",
    "    if parsed is None:\n",
    "        raise RuntimeError(\"No se encontró la tabla de 'Internacionales' en los Excel del ZIP.\")\n",
    "\n",
    "    parsed[\"Region\"] = parsed[\"Aerolíneas\"].astype(str).map(zstrip).map(REGION_KEYS.get)\n",
    "    col_prev, col_latest, col_part, latest_year = pick_latest_ytd_columns(parsed.columns.tolist())\n",
    "    if col_latest is None:\n",
    "        raise RuntimeError(\"No se detectó un bloque de columnas con año/YTD en los encabezados.\")\n",
    "\n",
    "    parsed[col_latest] = to_number(get_series(parsed, col_latest))\n",
    "    if col_prev and col_prev in parsed.columns:\n",
    "        parsed[col_prev] = to_number(get_series(parsed, col_prev))\n",
    "\n",
    "    part_series = _select_participation_series(parsed, latest_year) if col_part else None\n",
    "    if part_series is not None:\n",
    "        ser_str = part_series.astype(str).str.replace(\"%\", \"\", regex=False).str.replace(\",\", \".\", regex=False)\n",
    "        ser_num = pd.to_numeric(ser_str, errors=\"coerce\")\n",
    "        max_val = ser_num.dropna().max()\n",
    "        part = ser_num * 100.0 if (pd.notna(max_val) and max_val <= 1.0) else ser_num\n",
    "    else:\n",
    "        tmp = parsed[parsed[\"Region\"] != \"TOTAL\"]\n",
    "        latest_vals_tmp = to_number(get_series(tmp, col_latest))\n",
    "        s = latest_vals_tmp.sum()\n",
    "        latest_vals_all = to_number(get_series(parsed, col_latest))\n",
    "        part = (latest_vals_all / s * 100.0) if (s and np.isfinite(s) and s > 0) else pd.Series(np.nan, index=parsed.index)\n",
    "\n",
    "    prev_year = extract_year(col_prev) if col_prev else None\n",
    "\n",
    "    out = parsed.copy()\n",
    "    if prev_year:\n",
    "        out[f\"{prev_year}_YTD\"] = out[col_prev]\n",
    "    out[f\"{latest_year}_YTD\"] = out[col_latest]\n",
    "    out[\"Participacion_%\"] = part\n",
    "    if prev_year:\n",
    "        denom = out[col_prev]\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            out[\"Variacion_rel_%\"] = (out[col_latest] - denom) / denom * 100.0\n",
    "\n",
    "    order = [\"Estadounidenses\",\"Mexicanas\",\"Canadienses\",\"Europeas\",\"Centro/Sudamericanas\",\"Asiáticas\",\"TOTAL\"]\n",
    "    cols = [\"Region\"]\n",
    "    if prev_year: cols.append(f\"{prev_year}_YTD\")\n",
    "    cols += [f\"{latest_year}_YTD\", \"Participacion_%\"]\n",
    "    if prev_year: cols.append(\"Variacion_rel_%\")\n",
    "    out = out.set_index(\"Region\").reindex(order).reset_index()[cols]\n",
    "\n",
    "    # ----------------- Pie con Plotly -----------------\n",
    "    pie_df = out[out[\"Region\"] != \"TOTAL\"].copy()\n",
    "    vals = pie_df[f\"{latest_year}_YTD\"].fillna(0).values\n",
    "    labels = pie_df[\"Region\"].values\n",
    "\n",
    "    ranks_desc = np.argsort(-vals)\n",
    "    rank_map = {idx: rank for rank, idx in enumerate(ranks_desc)}\n",
    "\n",
    "    colors = [\n",
    "        (COLOR_TOPS[rank_map[i]] if rank_map[i] < 3 else REST_PALETTE[(rank_map[i]-3) % len(REST_PALETTE)])\n",
    "        for i in range(len(vals))\n",
    "    ]\n",
    "    pulls = [0.06 if rank_map[i] == 0 else 0.0 for i in range(len(vals))]\n",
    "\n",
    "    fig = go.Figure(\n",
    "        go.Pie(\n",
    "            labels=labels,\n",
    "            values=vals,\n",
    "            textinfo=\"text\",\n",
    "            texttemplate=\"%{percent:.1%}\",\n",
    "            textposition=\"outside\",\n",
    "            textfont=dict(size=12, family=FONT_FAMILY_PLOTLY),\n",
    "            marker=dict(colors=colors, line=dict(color=\"white\", width=1)),\n",
    "            pull=pulls,\n",
    "            sort=False,\n",
    "            direction=\"clockwise\",\n",
    "            hovertemplate=\"<b>%{label}</b><br>%{value:,.0f} pasajeros<br>%{percent:.1%}<extra></extra>\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    m_period = re.search(r\"\\(([^)]*?)\\)\", col_latest)\n",
    "    periodo = (m_period.group(1).strip() if m_period else (YTD_TAG_RE.search(col_latest).group(0).upper() if YTD_TAG_RE.search(col_latest) else \"YTD\"))\n",
    "\n",
    "    fig.update_layout(\n",
    "        # 1. Título Centrado\n",
    "        title=dict(\n",
    "            text=f\"Participación {latest_year} pasajeros transportados internacionales<br>\",\n",
    "            x=0.5,\n",
    "            xanchor='center',\n",
    "            font=dict(size=18, family=FONT_FAMILY_PLOTLY)\n",
    "        ),\n",
    "        font=dict(family=FONT_FAMILY_PLOTLY, size=12),\n",
    "        legend=dict(\n",
    "            title=f\"Participación {latest_year}\",\n",
    "            orientation=\"v\",\n",
    "            y=0.5, yanchor=\"middle\",\n",
    "            x=1.02, xanchor=\"left\",\n",
    "            font=dict(size=12, family=FONT_FAMILY_PLOTLY),\n",
    "            tracegroupgap=6,\n",
    "            itemwidth=40\n",
    "        ),\n",
    "        # 2. Aumento de margen inferior (b) para la fuente\n",
    "        margin=dict(l=LEFT_MARGIN, r=140, t=100, b=100),\n",
    "        autosize=False,\n",
    "        width=FIG_WIDTH,\n",
    "        height=FIG_HEIGHT,\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    for tr in fig.data:\n",
    "        if isinstance(tr, go.Pie):\n",
    "            tr.domain = dict(x=[0.0, 0.88], y=[0.05, 0.95])\n",
    "\n",
    "    # 3. Anotación de Fuente (Inferior Izquierda)\n",
    "    fig.add_annotation(\n",
    "        text=\"Fuente: Secretaría de Turismo (Datatur)\",\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0,      \n",
    "        y=-0.1,   \n",
    "        showarrow=False,\n",
    "        xanchor='left',\n",
    "        yanchor='top',\n",
    "        font=dict(size=11, color=\"gray\", family=FONT_FAMILY_PLOTLY)\n",
    "    )\n",
    "\n",
    "    return fig, out, f\"{periodo} {latest_year}\"\n",
    "\n",
    "# ----------------- Ejecución Streamlit -----------------\n",
    "\n",
    "st.title(\"Internacionales por Aerolínea (Datatur)\")\n",
    "\n",
    "if st.button(\"Recargar datos (Datatur)\"):\n",
    "    st.cache_data.clear()\n",
    "\n",
    "try:\n",
    "    fig, out_df, periodo_str = run_datatur_analysis()\n",
    "\n",
    "    st.subheader(f\"Distribución de Pasajeros por Región — {periodo_str}\")\n",
    "    st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "    # 4. Tabla de datos debajo de la gráfica\n",
    "    st.markdown(\"### Tabla de Datos Detallada\")\n",
    "    st.dataframe(out_df, use_container_width=True)\n",
    "\n",
    "except RuntimeError as e:\n",
    "    st.error(f\"Error fatal al procesar los datos de Datatur: {e}\")\n",
    "except Exception as e:\n",
    "    st.error(f\"Ocurrió un error inesperado durante la ejecución: {e}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN5+JkQuXB/bbjrRNVJn+5W",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
